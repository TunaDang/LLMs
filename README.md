# GPT-2 and LLMs
Recreating the paper "Attention is All You Need" with a twist on OpenAI's GPT-2 ground up from spelled-out backpropagation to tokenizer to transformers. Credits to Andrej Kapathy's YouTube tutorials, couldn't have gotten this far without his guidance.
