# LLMs
Recreating the paper "Attention is All You Need" with a twist on OpenAI's GPT-2/3 ground up from spelled-out backpropagation to tokenizer to transformers.
